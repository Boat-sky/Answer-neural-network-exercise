---
layout: notes
hidden_category: nndl
title: Sigmoid neurons simulating perceptrons, part II
weight: 2
---

**Question:** Suppose we have the same setup as the last problem - a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that w⋅x+b≠0 for the input xx to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant c>0 Show that in the limit as c→∞ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when w⋅x+b=0 for one of the perceptrons?


***

We can show that the network behaves similar to a perceptron network by showing that each individual neuron behaves as sigmoid neuron.

For an individual sigmoid neuron the output is given as:

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable columnalign="right center left" rowspacing="3pt" columnspacing="0 thickmathspace" displaystyle="true">
    <mlabeledtr>
      <mtd id="mjx-eqn-4">
        <mtext>(4)</mtext>
      </mtd>
      <mtd>
        <mfrac>
          <mn>1</mn>
          <mrow>
            <mn>1</mn>
            <mo>+</mo>
            <mi>exp</mi>
            <mo>&#x2061;<!-- ⁡ --></mo>
            <mo stretchy="false">(</mo>
            <mo>&#x2212;<!-- − --></mo>
            <munder>
              <mo>&#x2211;<!-- ∑ --></mo>
              <mi>j</mi>
            </munder>
            <msub>
              <mi>w</mi>
              <mi>j</mi>
            </msub>
            <msub>
              <mi>x</mi>
              <mi>j</mi>
            </msub>
            <mo>&#x2212;<!-- − --></mo>
            <mi>b</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </mfrac>
        <mo>.</mo>
      </mtd>
    </mlabeledtr>
  </mtable>
</math>
Expanding the **w.x** dotproduct we can write z as:

 (w~1~ * x~1~ + w~2~ * x~2~ + w~3~ * x~3~ .. ) + b

Now multiplying each weight and bias by constant C > 0

(c * w~1~ * x~1~ + c * w~2~ * x~2~ + c * w~3~ * x~3~ .. ) + c * b

Taking c common the expression reduces to.

 c * ((w~1~ * x~1~ + w~2~ * x~2~ + w~3~ * x~3~ .. ) + b)

Now for given conditions where z≠0 as c→∞:

if z > 0 then the exponential part of 4 reduces to zero in the limit. The output of the sigmoid neuron is 1.

if z < 0 then the exponential part of 4 tends to ∞ in the limit c→∞. The denminator is now >> numerator. The output of the neurons tends to 0.

This is exactly like the perceptron.

Except when z=0 then the output is always 0.5 which is not possible for a perceptron.





