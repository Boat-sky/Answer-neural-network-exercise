---
layout: notes
hidden_category: nndl
title: Sigmoid neurons simulating perceptrons, part I
weight: 1
---

**Question:** Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, c>0. Show that the behaviour of the network doesn't change.


***

We can show that the behaviour of the network does not change by showing that each individual neuron behaves in the orignal manner.

For an individual perceptron neuron the output is given as:

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable columnalign="right center left" rowspacing="3pt" columnspacing="0 thickmathspace" displaystyle="true">
    <mlabeledtr>
      <mtd id="mjx-eqn-2">
        <mtext>(2)</mtext>
      </mtd>
      <mtd>
        <mstyle displaystyle="false" scriptlevel="0">
          <mtext>output</mtext>
        </mstyle>
        <mo>=</mo>
        <mrow>
          <mo>{</mo>
          <mtable columnalign="left left" rowspacing="4pt" columnspacing="1em">
            <mtr>
              <mtd>
                <mn>0</mn>
              </mtd>
              <mtd>
                <mstyle displaystyle="false" scriptlevel="0">
                  <mtext>if&#xA0;</mtext>
                </mstyle>
                <mi>w</mi>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <mi>x</mi>
                <mo>+</mo>
                <mi>b</mi>
                <mo>&#x2264;<!-- ≤ --></mo>
                <mn>0</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>1</mn>
              </mtd>
              <mtd>
                <mstyle displaystyle="false" scriptlevel="0">
                  <mtext>if&#xA0;</mtext>
                </mstyle>
                <mi>w</mi>
                <mo>&#x22C5;<!-- ⋅ --></mo>
                <mi>x</mi>
                <mo>+</mo>
                <mi>b</mi>
                <mo>&gt;</mo>
                <mn>0</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo fence="true" stretchy="true" symmetric="true"></mo>
        </mrow>
      </mtd>
    </mlabeledtr>
  </mtable>
</math>

Expanding the **w.x** dotproduct we can write the LHS of the equality as:

 (w~1~ * x~1~ + w~2~ * x~2~ + w~3~ * x~3~ .. ) + b

Now multiplying each weight and bias by constant C > 0

(c * w~1~ * x~1~ + c * w~2~ * x~2~ + c * w~3~ * x~3~ .. ) + c * b

Taking c common the expression reduces to.

 c * ((w~1~ * x~1~ + w~2~ * x~2~ + w~3~ * x~3~ .. ) + b)

i.e we are multiplying the LHS of the equation by a positive constant c.

This would not alter the sign of the LHS which is the decidng factor for the output.

Hence each perceptron will give the same output for its given inputs. Thus overall network output will remain unchanged.


